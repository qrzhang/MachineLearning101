{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "> What distinguishes a machine learning algorithm from a non machine learning algorithm,  is its ability to __adapt__ its behaviours to new input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning\n",
    "data sample contain a target attribute (__ground truth__) <br>\n",
    "F(X) ≈ y\n",
    ">for a task of predicting the category of iris flower with the labeled data, one can tell that it is a supervised learning task. \n",
    "\n",
    "- Classification <br>\n",
    "  expected output of model ~ `discrete` value \n",
    "- Regression <br>\n",
    "  expected output of model ~ `continuous` value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning\n",
    "One is expected to learn the underlying patterns or rules from the data, without having the predefined ground truth as the benchmark.\n",
    "- __Clustering__ <br>\n",
    "  given a data set, one can cluster the samples into groups, based on the similarities among the samples within the data set. \n",
    "- __Association__ <br>\n",
    "  given a data set, the association task is to uncover the hidden association patterns among the attributes of a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised Learning\n",
    "In a scenario where the data set is massive but the labeled sample are few, one might find the application of both supervised and unsupervised learning\n",
    "> For example, one would like to predict the label of images, but only 10% of the images are labeled. By applying supervised learning, we train a model with the labeled data, then we apply the model to predict the unlabeled data. It would be hard to convince ourselves that the model would be general enough, after all we learnt from only the minority of data set. __A better strategy__ could be to first cluster the images into groups (unsupervised learning), and then apply the supervised learning algorithm on each of the groups individually. The unsupervised learning in the first stage could help us to narrow down the scope of learning so that the supervised learning in the second stage could obtain better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Vs. Regression\n",
    "> If the output of a machine learning model is `discrete values`, e.g. a boolean value, we then call it a __classification model__. While we call the model that outputs `continuous values` as __regression model__.\n",
    "\n",
    "<img src=\"https://assets.leetcode.com/uploads/2018/12/16/card_classification_with_coordinates.png\" width=\"500\" alt=\"Classification\" title=\"Classification\" /><img src=\"https://assets.leetcode.com/uploads/2018/12/16/card_regression.png\" alt=\"Regression\" width=\"500\" title=\"Regression\" />\n",
    "\n",
    "Speaking of `features`, we would also like to mention that some of the machine learning models (e.g. decision tree) can handle directly the `non-numeric` feature as it is, while more often one has to transform those non-numeric features into numeric one way or another.\n",
    "<br><br>\n",
    "Sometimes the boundary between these two models is not clear, and one can transform a classification problem into a regression problem, and vice versa. \n",
    "\n",
    "> `Logistic Regression`, which gives continuous probability values as output, but is served to solve the classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, Data, Data!\n",
    "> Rule of thumb: garbage in, garbage out.\n",
    "\n",
    "In the real world, the data we got reflects a part of reality in a favorable case, or it could be some noise in a less favorable case, or in worst case, even a contradiction to the reality. Regardless of the machine learning algorithms, one would not be able to learn anything from data that contains too much noise or is too inconsistent with the reality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow\n",
    "## Data-Centric Workflow\n",
    "<img src=\"https://assets.leetcode.com/uploads/2018/11/25/ml_workflow.png\" width=\"500\" alt=\"Classification\" title=\"Classification\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. determine the type of __learning problems__, supervised or unsupervised\n",
    "\n",
    "2. determine the type of __generated model__, classfication or regression\n",
    "\n",
    "3. __Feature Engineering__\n",
    "> The process of feature engineering is not a one-off step. Often one needs to repeatedly come back to the feature engineering later in the workflow.\n",
    "\n",
    "- `Split` the data into tw groups: training and testing\n",
    "- for `incomplete values`, one might need to fill those missing values with various strategies such as filling with the average value. \n",
    "- `Encode` those categorical string values into numerical one, due to the constraints of algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Training process__ <br>\n",
    "5. __Testing process__ <br>\n",
    "6. __hyper-parameter tuning__: One would then go back to the training process and tune some parameters that are exposed by the model that we selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Underfitting VS. Overfitting\n",
    "> An important measurement for supervised learning algorithms, is the __generalization__, which measures how well that a model derived from the training data can predict the desired attribute of the unseen data. When we say a model is underfitting or overfitting, it implies that the model does not generalized well to the unseen data. \n",
    "\n",
    "<img src=\"https://assets.leetcode.com/uploads/2019/01/01/underfitting.png\" width=\"500\" alt=\"Classification\" title=\"Classification\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Underfitting__ <br>\n",
    "    An underfitting model is the one which does not fit well with the training data, i.e. significantly deviated from the ground truth. One of the causes of underfitting could be that the model is over-simplified for the data, therefore it is not capable to capture the hidden relationship within the data.\n",
    "2. __Overfitting__ <br>\n",
    "    An overfitting model is the one which fits well with the training data, i.e. little or no error, however does not generalized well to the unseen data.<br>\n",
    "    To avoid the overfitting, one can try out another algorithm that could generate a simpler model from the training data set. Or more often, one stays with the original algorithm that generated the overfitting model, but adds a `regularization term` to the algorithm, i.e. penalising the model that is over-complicated so that the algorithm is steered to generate a less complicated model while fitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Vs. Variance\n",
    "> __Bias__ is a learner’s tendency to consistently learn the same __wrong__ thing. __Variance__ is the tendency to learn __random__ things unrelated to the real signal\n",
    "\n",
    "__Loss function__\n",
    "<img src=\"loss_function.png\" width=\"600\" alt=\"loss_function\" title=\"loss_function\" />\n",
    "\n",
    "__Main Prediction__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
